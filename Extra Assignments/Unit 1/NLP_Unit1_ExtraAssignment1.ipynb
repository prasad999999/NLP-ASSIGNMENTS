{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmuAfAgdPW1H"
      },
      "outputs": [],
      "source": [
        "# Lab Assignment 1: Text Preprocessing and Regular Expressions\n",
        "# •\tImplement tokenization, stemming, and lemmatization using NLTK and spaCy.\n",
        "# •\tUse regular expressions for tasks such as extracting email addresses, phone numbers, and hashtags from a given text dataset of minimum 5 pages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlqDxTxfRWs3",
        "outputId": "f81004c6-a94f-458f-f05c-91e1691c428e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "RgkWRtMIRaUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLC1PIq6RfqA",
        "outputId": "272db53e-1be9-4487-8724-0d57508eee82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "V-EqDrHSRksu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Simulate a sample 5-page dataset (you can load your own .txt or .csv here)\n",
        "text_data = \"\"\"\n",
        "Contact me at john.doe@example.com or jane_doe22@sample.org.\n",
        "My phone number is +1-800-555-1234 or (212) 555-4567.\n",
        "I love #MachineLearning and #AI!\n",
        "Barack Obama was the 44th president of the United States.\n",
        "SpaCy is great for NLP. NLTK is also useful.\n",
        "\n",
        "Email me at test.email@gmail.com or hello@mydomain.org.\n",
        "Call me at 987-654-3210 or 1234567890.\n",
        "Follow #Python and #DataScience on Twitter.\n",
        "The cat sat on the mat. The cats are sitting on the mats.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess into lines like different pages for simulation\n",
        "pages = text_data.strip().split('\\n')"
      ],
      "metadata": {
        "id": "qaPzRAs8Rm2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Tokenization ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    tokens = word_tokenize(page)\n",
        "    print(f\"\\nPage {i+1} Tokens:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu6JFZcMRpl-",
        "outputId": "50b0919c-5dd4-4748-f340-894dbead3c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tokenization ===\n",
            "\n",
            "Page 1 Tokens:\n",
            " ['Contact', 'me', 'at', 'john.doe', '@', 'example.com', 'or', 'jane_doe22', '@', 'sample.org', '.']\n",
            "\n",
            "Page 2 Tokens:\n",
            " ['My', 'phone', 'number', 'is', '+1-800-555-1234', 'or', '(', '212', ')', '555-4567', '.']\n",
            "\n",
            "Page 3 Tokens:\n",
            " ['I', 'love', '#', 'MachineLearning', 'and', '#', 'AI', '!']\n",
            "\n",
            "Page 4 Tokens:\n",
            " ['Barack', 'Obama', 'was', 'the', '44th', 'president', 'of', 'the', 'United', 'States', '.']\n",
            "\n",
            "Page 5 Tokens:\n",
            " ['SpaCy', 'is', 'great', 'for', 'NLP', '.', 'NLTK', 'is', 'also', 'useful', '.']\n",
            "\n",
            "Page 6 Tokens:\n",
            " []\n",
            "\n",
            "Page 7 Tokens:\n",
            " ['Email', 'me', 'at', 'test.email', '@', 'gmail.com', 'or', 'hello', '@', 'mydomain.org', '.']\n",
            "\n",
            "Page 8 Tokens:\n",
            " ['Call', 'me', 'at', '987-654-3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Tokens:\n",
            " ['Follow', '#', 'Python', 'and', '#', 'DataScience', 'on', 'Twitter', '.']\n",
            "\n",
            "Page 10 Tokens:\n",
            " ['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'The', 'cats', 'are', 'sitting', 'on', 'the', 'mats', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"\\n=== Stemming ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    tokens = word_tokenize(page)\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    print(f\"\\nPage {i+1} Stemmed:\\n\", stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1OtcIH_RsiV",
        "outputId": "fcdb5ca8-8c6c-43cb-9c70-28f916415b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Stemming ===\n",
            "\n",
            "Page 1 Stemmed:\n",
            " ['contact', 'me', 'at', 'john.do', '@', 'example.com', 'or', 'jane_doe22', '@', 'sample.org', '.']\n",
            "\n",
            "Page 2 Stemmed:\n",
            " ['my', 'phone', 'number', 'is', '+1-800-555-1234', 'or', '(', '212', ')', '555-4567', '.']\n",
            "\n",
            "Page 3 Stemmed:\n",
            " ['i', 'love', '#', 'machinelearn', 'and', '#', 'ai', '!']\n",
            "\n",
            "Page 4 Stemmed:\n",
            " ['barack', 'obama', 'wa', 'the', '44th', 'presid', 'of', 'the', 'unit', 'state', '.']\n",
            "\n",
            "Page 5 Stemmed:\n",
            " ['spaci', 'is', 'great', 'for', 'nlp', '.', 'nltk', 'is', 'also', 'use', '.']\n",
            "\n",
            "Page 6 Stemmed:\n",
            " []\n",
            "\n",
            "Page 7 Stemmed:\n",
            " ['email', 'me', 'at', 'test.email', '@', 'gmail.com', 'or', 'hello', '@', 'mydomain.org', '.']\n",
            "\n",
            "Page 8 Stemmed:\n",
            " ['call', 'me', 'at', '987-654-3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Stemmed:\n",
            " ['follow', '#', 'python', 'and', '#', 'datasci', 'on', 'twitter', '.']\n",
            "\n",
            "Page 10 Stemmed:\n",
            " ['the', 'cat', 'sat', 'on', 'the', 'mat', '.', 'the', 'cat', 'are', 'sit', 'on', 'the', 'mat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Lemmatization (spaCy) ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    doc = nlp(page)\n",
        "    lemmatized = [token.lemma_ for token in doc]\n",
        "    print(f\"\\nPage {i+1} Lemmatized:\\n\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kJZ4b-_Ru30",
        "outputId": "8db7a0c3-5399-4ebd-dc37-9e7a67afe95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Lemmatization (spaCy) ===\n",
            "\n",
            "Page 1 Lemmatized:\n",
            " ['contact', 'I', 'at', 'john.doe@example.com', 'or', 'jane_doe22@sample.org', '.']\n",
            "\n",
            "Page 2 Lemmatized:\n",
            " ['my', 'phone', 'number', 'be', '+1', '-', '800', '-', '555', '-', '1234', 'or', '(', '212', ')', '555', '-', '4567', '.']\n",
            "\n",
            "Page 3 Lemmatized:\n",
            " ['I', 'love', '#', 'MachineLearning', 'and', '#', 'AI', '!']\n",
            "\n",
            "Page 4 Lemmatized:\n",
            " ['Barack', 'Obama', 'be', 'the', '44th', 'president', 'of', 'the', 'United', 'States', '.']\n",
            "\n",
            "Page 5 Lemmatized:\n",
            " ['SpaCy', 'be', 'great', 'for', 'NLP', '.', 'NLTK', 'be', 'also', 'useful', '.']\n",
            "\n",
            "Page 6 Lemmatized:\n",
            " []\n",
            "\n",
            "Page 7 Lemmatized:\n",
            " ['email', 'I', 'at', 'test.email@gmail.com', 'or', 'hello@mydomain.org', '.']\n",
            "\n",
            "Page 8 Lemmatized:\n",
            " ['call', 'I', 'at', '987', '-', '654', '-', '3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Lemmatized:\n",
            " ['follow', '#', 'Python', 'and', '#', 'DataScience', 'on', 'Twitter', '.']\n",
            "\n",
            "Page 10 Lemmatized:\n",
            " ['the', 'cat', 'sit', 'on', 'the', 'mat', '.', 'the', 'cat', 'be', 'sit', 'on', 'the', 'mat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "phone_pattern = r'(\\+?\\d{1,3})?[\\s\\-\\.]?\\(?\\d{2,4}\\)?[\\s\\-\\.]?\\d{3,4}[\\s\\-\\.]?\\d{4}'\n",
        "hashtag_pattern = r'#\\w+'\n",
        "\n",
        "print(\"\\n=== Regex Extraction ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    emails = re.findall(email_pattern, page)\n",
        "    phones = re.findall(phone_pattern, page)\n",
        "    hashtags = re.findall(hashtag_pattern, page)\n",
        "\n",
        "    print(f\"\\nPage {i+1} Results:\")\n",
        "    print(\"Emails:\", emails)\n",
        "    print(\"Phone Numbers:\", phones)\n",
        "    print(\"Hashtags:\", hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jOU1vxmRxj3",
        "outputId": "3ea67b60-8d93-4f78-ec78-a4e6c1b3628b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Regex Extraction ===\n",
            "\n",
            "Page 1 Results:\n",
            "Emails: ['john.doe@example.com', 'jane_doe22@sample.org.']\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 2 Results:\n",
            "Emails: []\n",
            "Phone Numbers: ['+1', '']\n",
            "Hashtags: []\n",
            "\n",
            "Page 3 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: ['#MachineLearning', '#AI']\n",
            "\n",
            "Page 4 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 5 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 6 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 7 Results:\n",
            "Emails: ['test.email@gmail.com', 'hello@mydomain.org.']\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 8 Results:\n",
            "Emails: []\n",
            "Phone Numbers: ['', '']\n",
            "Hashtags: []\n",
            "\n",
            "Page 9 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: ['#Python', '#DataScience']\n",
            "\n",
            "Page 10 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n"
          ]
        }
      ]
    }
  ]
}