{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPLmxotBX2fD"
      },
      "outputs": [],
      "source": [
        "# Lab Assignment 5: N-gram Language Model\n",
        "# •\tImplement unigram, bigram, and trigram models using NLTK.\n",
        "# •\tTrain on a small text dataset and compute probabilities of word sequences.\n",
        "# •\tUse Laplace smoothing to handle unseen words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and import libraries\n",
        "!pip install -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZrXQn7xX_Ph",
        "outputId": "f9a239d2-7806-4b07-f086-2708d7ad78d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "id": "_Qz2-ZxsYXwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR6_TnmMYYi4",
        "outputId": "7a2fa21e-8d69-4bfb-fad3-bfac160e044b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Sample small dataset (you can extend this or load from file)\n",
        "text = \"\"\"\n",
        "Natural language processing enables computers to understand human language.\n",
        "It involves linguistics and machine learning. Language models are essential in NLP.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y1wqx0EnYbgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Tokenization and preprocessing\n",
        "tokens = word_tokenize(text.lower())\n",
        "tokens = ['<s>'] + tokens + ['</s>']  # Start and end tokens"
      ],
      "metadata": {
        "id": "oxrlYXlvYd-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build Unigram, Bigram, and Trigram Models\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "vocab = set(tokens)\n",
        "V = len(vocab)  # Vocabulary size"
      ],
      "metadata": {
        "id": "o24sLAQ3YgsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define Probability Functions with Laplace Smoothing\n",
        "\n",
        "def unigram_prob(word):\n",
        "    return (unigram_counts[(word,)] + 1) / (sum(unigram_counts.values()) + V)\n",
        "\n",
        "def bigram_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[(w1,)] + V)\n",
        "\n",
        "def trigram_prob(w1, w2, w3):\n",
        "    return (trigram_counts[(w1, w2, w3)] + 1) / (bigram_counts[(w1, w2)] + V)"
      ],
      "metadata": {
        "id": "cgoWzyuFYpU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Compute Probabilities of Sample Sequences\n",
        "\n",
        "def compute_sequence_prob(sequence):\n",
        "    tokens = ['<s>'] + word_tokenize(sequence.lower()) + ['</s>']\n",
        "\n",
        "    print(f\"\\nSequence: {sequence}\")\n",
        "\n",
        "    # Unigram\n",
        "    uni_prob = 1.0\n",
        "    for w in tokens:\n",
        "        prob = unigram_prob(w)\n",
        "        uni_prob *= prob\n",
        "    print(f\"Unigram Probability: {uni_prob:.10f} | LogProb: {math.log(uni_prob):.4f}\")\n",
        "\n",
        "    # Bigram\n",
        "    bi_prob = 1.0\n",
        "    for w1, w2 in ngrams(tokens, 2):\n",
        "        prob = bigram_prob(w1, w2)\n",
        "        bi_prob *= prob\n",
        "    print(f\"Bigram Probability: {bi_prob:.10f} | LogProb: {math.log(bi_prob):.4f}\")\n",
        "\n",
        "    # Trigram\n",
        "    tri_prob = 1.0\n",
        "    for w1, w2, w3 in ngrams(tokens, 3):\n",
        "        prob = trigram_prob(w1, w2, w3)\n",
        "        tri_prob *= prob\n",
        "    print(f\"Trigram Probability: {tri_prob:.10f} | LogProb: {math.log(tri_prob):.4f}\")"
      ],
      "metadata": {
        "id": "yP6B-_NkYs-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Test the model on a sample input\n",
        "compute_sequence_prob(\"language models are essential\")\n",
        "compute_sequence_prob(\"computers learn language\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emyUew2jY5uX",
        "outputId": "6cd9ad3a-f5db-4ade-b940-efee5068d006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequence: language models are essential\n",
            "Unigram Probability: 0.0000000105 | LogProb: -18.3752\n",
            "Bigram Probability: 0.0000011435 | LogProb: -13.6814\n",
            "Trigram Probability: 0.0000149436 | LogProb: -11.1112\n",
            "\n",
            "Sequence: computers learn language\n",
            "Unigram Probability: 0.0000001256 | LogProb: -15.8903\n",
            "Bigram Probability: 0.0000034370 | LogProb: -12.5809\n",
            "Trigram Probability: 0.0000939144 | LogProb: -9.2731\n"
          ]
        }
      ]
    }
  ]
}