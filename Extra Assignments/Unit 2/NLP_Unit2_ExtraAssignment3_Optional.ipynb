{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V41epdIrbmPr"
      },
      "outputs": [],
      "source": [
        "# Lab Assignment 3: Morphological Analysis with Finite State Transducers (FST) and Deep Learning\n",
        "# •\tImplement a Finite State Transducer (FST) for morphological parsing (e.g., handling verb conjugations and noun declensions in an Indian language like Hindi or Sanskrit).\n",
        "# •\tTrain a sequence-to-sequence deep learning model (LSTM-based) to predict morphemes for unseen words.\n",
        "# •\tCompare performance between FST and deep learning approaches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Install Dependencies\n",
        "!pip install nltk torch torchtext matplotlib --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQRs9vyObvBW",
        "outputId": "f3033e4b-6595-4e06-94f1-574ba19823b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: FST-Based Morphological Parser (Simple Demo)\n",
        "def fst_morphological_parser(word):\n",
        "    # Basic rule-based transducer\n",
        "    rules = {\n",
        "        'ing': '',     # running -> run\n",
        "        'ed': '',      # jumped -> jump\n",
        "        's': '',       # dogs -> dog\n",
        "        'er': '',      # taller -> tall\n",
        "    }\n",
        "\n",
        "    for suffix, root_suffix in rules.items():\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)] + root_suffix, suffix\n",
        "    return word, ''  # no transformation\n",
        "\n",
        "# Test FST\n",
        "words = ['running', 'jumped', 'dogs', 'taller', 'run']\n",
        "print(\"FST Morphological Analysis:\")\n",
        "for word in words:\n",
        "    root, suffix = fst_morphological_parser(word)\n",
        "    print(f\"{word} -> root: {root}, suffix: {suffix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uofRH2b9og",
        "outputId": "b474b01d-55af-43a4-a2b9-2231e2d9fb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FST Morphological Analysis:\n",
            "running -> root: runn, suffix: ing\n",
            "jumped -> root: jump, suffix: ed\n",
            "dogs -> root: dog, suffix: s\n",
            "taller -> root: tall, suffix: er\n",
            "run -> root: run, suffix: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Sequence-to-Sequence Model (LSTM) for Morpheme Prediction\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "Hdk7FckocDtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create toy dataset\n",
        "dataset = [\n",
        "    ('running', 'run ing'),\n",
        "    ('jumped', 'jump ed'),\n",
        "    ('taller', 'tall er'),\n",
        "    ('dogs', 'dog s'),\n",
        "    ('played', 'play ed'),\n",
        "]"
      ],
      "metadata": {
        "id": "wSbTkiodcJYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level tokenizer\n",
        "def tokenize(word):\n",
        "    return list(word)"
      ],
      "metadata": {
        "id": "DxLCUPRJcL6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary\n",
        "all_chars = sorted(set(\"\".join(w for w, _ in dataset) + \" \".join(m for _, m in dataset)))\n",
        "char2idx = {c: i + 1 for i, c in enumerate(all_chars)}  # +1 to reserve 0 for padding\n",
        "idx2char = {i: c for c, i in char2idx.items()}"
      ],
      "metadata": {
        "id": "m5MUrQ4jcONI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "def encode(seq):\n",
        "    return torch.tensor([char2idx[c] for c in seq], dtype=torch.long)"
      ],
      "metadata": {
        "id": "EMxywBeucYnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MorphDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        return encode(tokenize(src)), encode(tokenize(tgt))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    srcs = pad_sequence(srcs, batch_first=True)\n",
        "    tgts = pad_sequence(tgts, batch_first=True)\n",
        "    return srcs, tgts\n",
        "\n",
        "train_loader = DataLoader(MorphDataset(dataset), batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Seq2Seq model with LSTM\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=32, hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size + 1, embed_size, padding_idx=0)\n",
        "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size + 1)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        embedded_src = self.embed(src)\n",
        "        _, (h, c) = self.encoder(embedded_src)\n",
        "\n",
        "        embedded_tgt = self.embed(tgt)\n",
        "        out, _ = self.decoder(embedded_tgt, (h, c))\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2Seq(len(char2idx)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = loss_fn(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbEP01TFcQyi",
        "outputId": "fdb2e7d6-9468-47d5-c19e-bfcd35cbd740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.5957\n",
            "Epoch 2, Loss: 7.3428\n",
            "Epoch 3, Loss: 6.3776\n",
            "Epoch 4, Loss: 4.4763\n",
            "Epoch 5, Loss: 3.1496\n",
            "Epoch 6, Loss: 2.1778\n",
            "Epoch 7, Loss: 1.9020\n",
            "Epoch 8, Loss: 1.2863\n",
            "Epoch 9, Loss: 0.7752\n",
            "Epoch 10, Loss: 0.5774\n",
            "Epoch 11, Loss: 0.4509\n",
            "Epoch 12, Loss: 0.3277\n",
            "Epoch 13, Loss: 0.2757\n",
            "Epoch 14, Loss: 0.1757\n",
            "Epoch 15, Loss: 0.1704\n",
            "Epoch 16, Loss: 0.1005\n",
            "Epoch 17, Loss: 0.0847\n",
            "Epoch 18, Loss: 0.0751\n",
            "Epoch 19, Loss: 0.0571\n",
            "Epoch 20, Loss: 0.0449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Evaluate the LSTM Morphological Analyzer\n",
        "def predict(model, word):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = encode(tokenize(word)).unsqueeze(0).to(device)\n",
        "        embedded_src = model.embed(src)\n",
        "        _, (h, c) = model.encoder(embedded_src)\n",
        "\n",
        "        input_tgt = torch.tensor([[char2idx[' ']]], device=device)\n",
        "        output_seq = []\n",
        "\n",
        "        for _ in range(15):  # max output length\n",
        "            embedded_tgt = model.embed(input_tgt)\n",
        "            out, (h, c) = model.decoder(embedded_tgt, (h, c))\n",
        "            logits = model.fc(out[:, -1, :])\n",
        "            predicted = logits.argmax(dim=-1)\n",
        "            char = idx2char.get(predicted.item(), '')\n",
        "            if char == '':\n",
        "                break\n",
        "            output_seq.append(char)\n",
        "            input_tgt = predicted.unsqueeze(0)\n",
        "        return ''.join(output_seq)"
      ],
      "metadata": {
        "id": "WO3Z6HkbcakP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction\n",
        "test_words = ['running', 'taller', 'dogs']\n",
        "print(\"\\nLSTM Predictions:\")\n",
        "for word in test_words:\n",
        "    print(f\"{word} -> {predict(model, word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ZvY8QachZ_",
        "outputId": "8a96ccdc-305f-45c1-dca0-74eb7ab7333b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM Predictions:\n",
            "running -> un ing ing sssr\n",
            "taller -> all errrrrg srs\n",
            "dogs -> og ssssrsrsrsrs\n"
          ]
        }
      ]
    }
  ]
}